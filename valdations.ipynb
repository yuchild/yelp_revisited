{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d6ec07-e16a-4602-8341-8712315395a8",
   "metadata": {},
   "source": [
    "### Yelp Dataset 10Mar2025 Validations or Proof of Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4df74-665f-476e-8548-7bcb4c1a3d26",
   "metadata": {},
   "source": [
    "#### 1. Extrat Tar File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1590e9a-f422-4371-84f2-d796295769f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90f223-dd68-43b8-85fc-2edaa13b3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Define the path to the tar file and an extraction directory\n",
    "tar_path = '/media/oem/onetbsamdot/datasets/yelp_10Mar2025/Yelp-JSON/Yelp JSON/yelp_dataset.tar'\n",
    "extract_path = '/media/oem/onetbsamdot/datasets/yelp_10Mar2025/Yelp-JSON/Yelp JSON/'  # or another directory of your choice\n",
    "\n",
    "# Create the extraction directory if it doesn't exist\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "# Open and extract the tar file\n",
    "# with tarfile.open(tar_path, 'r') as tar:\n",
    "#     tar.extractall(path=extract_path)\n",
    "\n",
    "# List the extracted files to verify\n",
    "print(os.listdir(extract_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b147bc6-baf6-46a5-9fed-37031a9fc63c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyRapidsSparkApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\", \"/home/oem/jars/rapids-4-spark_2.12-25.02.0.jar\") \\\n",
    "    .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\") \\\n",
    "    .config(\"spark.rapids.sql.enabled\", \"true\") \\\n",
    "    .config(\"spark.executor.resource.gpu.amount\", \"1\") \\\n",
    "    .config(\"spark.task.resource.gpu.amount\", \"1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"RAPIDS enabled:\", spark.conf.get(\"spark.rapids.sql.enabled\", \"Not set\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda3922-7c60-4c6e-bb8e-6265502c09c0",
   "metadata": {},
   "source": [
    "#### 2. Import JSON Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447ca7c-ae4e-4d29-a23e-3999e934681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths (ensure extract_path is defined)\n",
    "business_file = os.path.join(extract_path, 'yelp_academic_dataset_business.json')\n",
    "review_file   = os.path.join(extract_path, 'yelp_academic_dataset_review.json')\n",
    "checkin_file  = os.path.join(extract_path, 'yelp_academic_dataset_checkin.json')\n",
    "tip_file      = os.path.join(extract_path, 'yelp_academic_dataset_tip.json')\n",
    "user_file     = os.path.join(extract_path, 'yelp_academic_dataset_user.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d792e-6f1e-4b27-b9a4-a29a279c1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor # leverage mutli-core processes for parallel compute\n",
    "\n",
    "# Create a directory to store the converted Parquet files\n",
    "parquet_dir = \"./data\"\n",
    "os.makedirs(parquet_dir, exist_ok=True)\n",
    "\n",
    "# Function to convert a JSON file to Parquet using Pandas\n",
    "def convert_json_to_parquet(json_path, parquet_path):\n",
    "    print(f\"Converting {json_path} to {parquet_path}...\")\n",
    "    # Read the JSON file (JSON Lines mode)\n",
    "    df = pd.read_json(json_path, lines=True)\n",
    "    # Write the DataFrame to Parquet (without the index)\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "    print(f\"Saved {parquet_path}\")\n",
    "\n",
    "\n",
    "# List the conversion tasks as (source, destination) tuples\n",
    "tasks = [\n",
    "    (business_file, os.path.join(parquet_dir, 'business.parquet')),\n",
    "    (review_file, os.path.join(parquet_dir, 'review.parquet')),\n",
    "    (checkin_file, os.path.join(parquet_dir, 'checkin.parquet')),\n",
    "    (tip_file, os.path.join(parquet_dir, 'tip.parquet')),\n",
    "    (user_file, os.path.join(parquet_dir, 'user.parquet')),\n",
    "]\n",
    "\n",
    "# Use a ProcessPoolExecutor with 5 workers to convert files in parallel\n",
    "with ProcessPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(convert_json_to_parquet, src, dest) for src, dest in tasks]\n",
    "    # Optionally wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        future.result()\n",
    "\n",
    "print(\"All conversions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce56855-9e94-4b0e-936a-c0928c327f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each Parquet file into a Spark DataFrame\n",
    "business_df = spark.read.parquet(os.path.join(parquet_dir, 'business.parquet')).sample(False, 0.1, seed=42)\n",
    "review_df   = spark.read.parquet(os.path.join(parquet_dir, 'review.parquet')).sample(False, 0.1, seed=42)\n",
    "checkin_df  = spark.read.parquet(os.path.join(parquet_dir, 'checkin.parquet')).sample(False, 0.1, seed=42)\n",
    "tip_df      = spark.read.parquet(os.path.join(parquet_dir, 'tip.parquet')).sample(False, 0.1, seed=42)\n",
    "user_df     = spark.read.parquet(os.path.join(parquet_dir, 'user.parquet')).sample(False, 0.1, seed=42)\n",
    "\n",
    "# Show a sample from one DataFrame\n",
    "business_df.show(5)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc4fef-e3d2-40d8-b1b1-ce52b1d4eaef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ylpenv)",
   "language": "python",
   "name": "ylpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
